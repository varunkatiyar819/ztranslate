{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"/14td1/varun-18869/model/create_finetune/finetune/Samanantar.en-mr.en\", \"r\") as f1, open(\"/14td1/varun-18869/model/create_finetune/finetune/Samanantar.en-mr.mr\", \"r\") as f2:\n",
    "    src_lines = f1.readlines()\n",
    "    tgt_lines = f2.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"source\" : src_lines, \"target\": tgt_lines})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Today, ASEAN and India enjoy multi-faceted coo...</td>\n",
       "      <td>आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For us in India, ASEANs continuance in this ro...</td>\n",
       "      <td>आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>We need to put faith in his reminders. \\n</td>\n",
       "      <td>[ ७ पानांवरील चित्र] \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Presently elections are being held in Gujarat. \\n</td>\n",
       "      <td>दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This will aim to pool research and technologic...</td>\n",
       "      <td>यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  Today, ASEAN and India enjoy multi-faceted coo...   \n",
       "1  For us in India, ASEANs continuance in this ro...   \n",
       "2          We need to put faith in his reminders. \\n   \n",
       "3  Presently elections are being held in Gujarat. \\n   \n",
       "4  This will aim to pool research and technologic...   \n",
       "\n",
       "                                              target  \n",
       "0  आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...  \n",
       "1  आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...  \n",
       "2                            [ ७ पानांवरील चित्र] \\n  \n",
       "3  दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...  \n",
       "4  यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([], dtype='int64')\n",
      "Int64Index([], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "# Checking for NULL or Empty Values\n",
    "faulty_indices = df[df.isnull().any(axis=1)].index\n",
    "empty_indices = df[df.isna().any(axis=1)].index\n",
    "print(faulty_indices)\n",
    "print(empty_indices)\n",
    "\n",
    "cleaned_data = df.drop(index=faulty_indices)\n",
    "cleaned_data = df.drop(index=empty_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all \\n's from all the sentences.\n",
    "# Remove newline characters and converting everything to lowercase for source sentences to avoid increased vocab.\n",
    "df['source'] = df['source'].str.replace('\\n', '').str.lower()\n",
    "df['target'] = df['target'].str.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "# Removing those lines which contains words not part of their respective langauge.\n",
    "# Load pre-trained FastText language detection model\n",
    "model = fasttext.load_model('/14td1/varun-18869/model/models/fasttext_model/lid.176.bin')\n",
    "\n",
    "# Function to detect language\n",
    "def detect_language(text):\n",
    "    prediction = model.predict(text)\n",
    "    return prediction[0][0].replace('__label__', '')\n",
    "\n",
    "# Detect languages for source and target sentences\n",
    "df['source_language'] = df['source'].apply(detect_language) #Function called here\n",
    "df['target_language'] = df['target'].apply(detect_language) #Function called here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>today, asean and india enjoy multi-faceted coo...</td>\n",
       "      <td>आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for us in india, aseans continuance in this ro...</td>\n",
       "      <td>आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we need to put faith in his reminders.</td>\n",
       "      <td>[ ७ पानांवरील चित्र]</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presently elections are being held in gujarat.</td>\n",
       "      <td>दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this will aim to pool research and technologic...</td>\n",
       "      <td>यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  today, asean and india enjoy multi-faceted coo...   \n",
       "1  for us in india, aseans continuance in this ro...   \n",
       "2            we need to put faith in his reminders.    \n",
       "3    presently elections are being held in gujarat.    \n",
       "4  this will aim to pool research and technologic...   \n",
       "\n",
       "                                              target source_language  \\\n",
       "0  आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...              en   \n",
       "1  आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...              en   \n",
       "2                              [ ७ पानांवरील चित्र]               en   \n",
       "3  दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...              en   \n",
       "4  यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...              en   \n",
       "\n",
       "  target_language  \n",
       "0              mr  \n",
       "1              mr  \n",
       "2              mr  \n",
       "3              mr  \n",
       "4              mr  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where the detected language does not match the majority language\n",
    "#In the case we dont know source and target languages\n",
    "# Identify the majority language for source and target columns\n",
    "\"\"\"\n",
    "import Counter\n",
    "source_language_counts = Counter(df['source_language'])\n",
    "target_language_counts = Counter(df['target_language'])\n",
    "\n",
    "majority_source_language = source_language_counts.most_common(1)[0][0]\n",
    "majority_target_language = target_language_counts.most_common(1)[0][0]\n",
    "\"\"\"\n",
    "source_language = \"en\"\n",
    "target_language = \"mr\"\n",
    "\n",
    "cleaned_data = df[\n",
    "    (df['source_language'] == source_language) &\n",
    "    (df['target_language'] == target_language)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3575532"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3575532"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove duplicate source sentences along with their corresponding target sentences\n",
    "#Since we are not using de-duplication here due to small dataset.\n",
    "\"\"\"\n",
    "cleaned_data = cleaned_data.drop_duplicates(subset='source', keep=False)\n",
    "\"\"\"\n",
    "cleaned_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3598185/2428176987.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['source_char_count'] = cleaned_data['source'].str.len()\n",
      "/tmp/ipykernel_3598185/2428176987.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_data['target_char_count'] = cleaned_data['target'].str.len()\n"
     ]
    }
   ],
   "source": [
    "#Removing those lines having target sentence characters X2 or /2 means less or more than 2 times to that of source.\n",
    "\n",
    "# Calculate character counts for source and target sentences\n",
    "cleaned_data['source_char_count'] = cleaned_data['source'].str.len()\n",
    "cleaned_data['target_char_count'] = cleaned_data['target'].str.len()\n",
    "\n",
    "# Filter out rows where the target character count is either double or half of the source character count\n",
    "cleaned_data = cleaned_data[\n",
    "    (cleaned_data['target_char_count'] < 2 * cleaned_data['source_char_count']) |\n",
    "    (cleaned_data['target_char_count'] > cleaned_data['source_char_count'] / 2)\n",
    "]\n",
    "\n",
    "df  = cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>source_language</th>\n",
       "      <th>target_language</th>\n",
       "      <th>source_char_count</th>\n",
       "      <th>target_char_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>today, asean and india enjoy multi-faceted coo...</td>\n",
       "      <td>आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "      <td>301</td>\n",
       "      <td>401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>for us in india, aseans continuance in this ro...</td>\n",
       "      <td>आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "      <td>158</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we need to put faith in his reminders.</td>\n",
       "      <td>[ ७ पानांवरील चित्र]</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>presently elections are being held in gujarat.</td>\n",
       "      <td>दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "      <td>47</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>this will aim to pool research and technologic...</td>\n",
       "      <td>यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...</td>\n",
       "      <td>en</td>\n",
       "      <td>mr</td>\n",
       "      <td>183</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              source  \\\n",
       "0  today, asean and india enjoy multi-faceted coo...   \n",
       "1  for us in india, aseans continuance in this ro...   \n",
       "2            we need to put faith in his reminders.    \n",
       "3    presently elections are being held in gujarat.    \n",
       "4  this will aim to pool research and technologic...   \n",
       "\n",
       "                                              target source_language  \\\n",
       "0  आज आसियान आणि भारत, आसियानचे राजनीतिक- सुरक्षा...              en   \n",
       "1  आसियान राष्ट्र समुहातील देशांशी भारताचे उत्तम ...              en   \n",
       "2                              [ ७ पानांवरील चित्र]               en   \n",
       "3  दरम्यान, गुजरात निवडणूकीचा प्रचार आता शिगेला प...              en   \n",
       "4  यामुळे सौर ऊर्जा क्षेत्रातल्या संशोधनाला आणि त...              en   \n",
       "\n",
       "  target_language  source_char_count  target_char_count  \n",
       "0              mr                301                401  \n",
       "1              mr                158                175  \n",
       "2              mr                 39                 21  \n",
       "3              mr                 47                 58  \n",
       "4              mr                183                169  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting would the last activity to be perfomed before that dataset cleaning is done\n",
    "\n",
    "#Drop the language columns and character count columns as they are no longer needed\n",
    "df = df.drop(columns=['source_language', 'target_language', 'source_char_count', 'target_char_count'])\n",
    "\n",
    "# Splitting into train and test/validation\n",
    "train_data, test_valid_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Splitting test/validation into test and validation\n",
    "test_data, valid_data = train_test_split(test_valid_data, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating files for train test valid for each source and target\n",
    "train_source_path = \"/14td1/varun-18869/model/create_finetune/finetune/train_source.txt\"\n",
    "train_target_path = \"/14td1/varun-18869/model/create_finetune/finetune/train_target.txt\"\n",
    "test_source_path = \"/14td1/varun-18869/model/create_finetune/finetune/test_source.txt\"\n",
    "test_target_path = \"/14td1/varun-18869/model/create_finetune/finetune/test_target.txt\"\n",
    "valid_source_path = \"/14td1/varun-18869/model/create_finetune/finetune/valid_source.txt\"\n",
    "valid_target_path = \"/14td1/varun-18869/model/create_finetune/finetune/valid_target.txt\"\n",
    "\n",
    "# Save train, test, and validation datasets to text files\n",
    "train_data[['source']].to_csv(train_source_path, index=False, header=False)\n",
    "train_data[['target']].to_csv(train_target_path, index=False, header=False)\n",
    "\n",
    "test_data[['source']].to_csv(test_source_path, index=False, header=False)\n",
    "test_data[['target']].to_csv(test_target_path, index=False, header=False)\n",
    "\n",
    "valid_data[['source']].to_csv(valid_source_path, index=False, header=False)\n",
    "valid_data[['target']].to_csv(valid_target_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspm_train --input=total.txt --model_prefix=your_model --vocab_size=32000 --character_coverage=1.0 \\\\--model_type=bpe --input_sentence_size=300000000  --shuffle_input_sentence=true --user_defined_symbols=\"__email0__\",\"__email1__\",\"__email2__\",\"__email3__\",\"__email4__\",\"__url0__\",\"__url1__\",\"__url2__\"\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then comes the tokenization step.\n",
    "\"\"\"\n",
    "spm_train --input=total.txt --model_prefix=your_model --vocab_size=32000 --character_coverage=1.0 \\--model_type=bpe --input_sentence_size=300000000  --shuffle_input_sentence=true --user_defined_symbols=\"__email0__\",\"__email1__\",\"__email2__\",\"__email3__\",\"__email4__\",\"__url0__\",\"__url1__\",\"__url2__\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the dataset into pieces -> i.e tokenizing the dataset\n",
    "\n",
    "# spm_encode --model=your_model.model --output_format=piece < train_file.$src > spm_encoded_train.$src &\n",
    "# spm_encode --model=your_model.model --output_format=piece < train_file.$trg > spm_encoded_train.$trg &"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nnum_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\\n    fairseq-preprocess --source-lang en --target-lang hi              --trainpref train --validpref valid --testpref test              --destdir data-bin --nwordssrc 32000 --nwordstgt 32000 --workers $num_workers              --log-interval 1              --log-format tqdm '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Then Comes Preprocessing step for optimizations.\n",
    "\"\"\"\n",
    "num_workers=`python -c \"import multiprocessing; print(multiprocessing.cpu_count())\"`\n",
    "    fairseq-preprocess --source-lang en --target-lang hi \\\n",
    "             --trainpref train --validpref valid --testpref test \\\n",
    "             --destdir data-bin --nwordssrc 32000 --nwordstgt 32000 --workers $num_workers \\\n",
    "             --log-interval 1 \\\n",
    "             --log-format tqdm \\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nCUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 fairseq-train  data-bin/  --arch transformer --save-dir checkpoints_en_hi/           --log-interval 2          --log-format tqdm          --source-lang=en --target-lang=hi          --encoder-embed-dim 512  --encoder-ffn-embed-dim 1024 --encoder-attention-heads 8          --decoder-embed-dim 512  --decoder-ffn-embed-dim 1024 --decoder-attention-heads 8          --optimizer adam --adam-betas '(0.9, 0.98)'         --clip-norm 1.0 --lr 0.0005 --lr-scheduler inverse_sqrt  --warmup-updates 4000          --warmup-init-lr 1e-07 --dropout 0.3  --weight-decay 0.001   --criterion label_smoothed_cross_entropy           --label-smoothing 0.1 --max-tokens 1024 --fp16 --update-freq 16 --max-epoch 10 --eval-bleu          --eval-bleu-remove-bpe          --best-checkpoint-metric bleu --maximize-best-checkpoint-metric          --tensorboard-logdir data_en_hi/logs          --wandb-project transformer_en_hi          --seed 1234 --find-unused-parameters --skip-invalid-size-inputs-valid-test\\n\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then Comes the Training Step\n",
    "\"\"\"\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 fairseq-train  data-bin/  --arch transformer --save-dir checkpoints_en_hi/  \\\n",
    "         --log-interval 2 \\\n",
    "         --log-format tqdm \\\n",
    "         --source-lang=en --target-lang=hi \\\n",
    "         --encoder-embed-dim 512  --encoder-ffn-embed-dim 1024 --encoder-attention-heads 8 \\\n",
    "         --decoder-embed-dim 512  --decoder-ffn-embed-dim 1024 --decoder-attention-heads 8 \\\n",
    "         --optimizer adam --adam-betas '(0.9, 0.98)'\\\n",
    "         --clip-norm 1.0 --lr 0.0005 --lr-scheduler inverse_sqrt  --warmup-updates 4000 \\\n",
    "         --warmup-init-lr 1e-07 --dropout 0.3  --weight-decay 0.001   --criterion label_smoothed_cross_entropy  \\\n",
    "         --label-smoothing 0.1 --max-tokens 1024 --fp16 --update-freq 16 --max-epoch 10 --eval-bleu \\\n",
    "         --eval-bleu-remove-bpe \\\n",
    "         --best-checkpoint-metric bleu --maximize-best-checkpoint-metric \\\n",
    "         --tensorboard-logdir data_en_hi/logs \\\n",
    "         --wandb-project transformer_en_hi \\\n",
    "         --seed 1234 --find-unused-parameters --skip-invalid-size-inputs-valid-test\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then comes inference or generation step.\n",
    "\"\"\"\n",
    "CUDA_VISIBLE_DEVICES=\"4,5,6\" fairseq-interactive data-bin \\\n",
    "    --path ./model_output/checkpoint_best.pt \\\n",
    "    --task translation \\\n",
    "    --source-lang en \\\n",
    "    --target-lang hi \\\n",
    "    --beam 5 \\\n",
    "    --remove-bpe=sentencepiece \\\n",
    "    --buffer-size 128 \\\n",
    "    --batch-size 128 \\\n",
    "    --log-format tqdm \\\n",
    "    --log-interval 60 \\\n",
    "    --input input.txt \\\n",
    "    > output.txt\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking inference from above teacher model, Now to create student model from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fairseq-generate inferencing is done on student model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, for fast and optimized inference converting the model to ctranslate2 format.\n",
    "\"\"\"ct2-fairseq-converter --model_path model_output/checkpoint_best.pt --data_dir ./data-bin/ --source_lang en --target_lang mr --output_dir ct2_model --quantization int8 --force\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now getting inference from ctranslate2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fine tuning the above model for similar language to that of hindi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
